diff --git a/.gitignore b/.gitignore
index 0c39aa2..6cfe680 100644
--- a/.gitignore
+++ b/.gitignore
@@ -46,7 +46,7 @@ Module.symvers
 /tags
 /TAGS
 /linux
-/vmlinux
+/vmlinux*
 /vmlinux.32
 /vmlinux-gdb.py
 /vmlinuz
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 5aef183..f2a3c3f 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -341,6 +341,11 @@
 332	common	statx			sys_statx

 #
+# vNUMA system call numbers.
+#
+333	common	get_numa_info		sys_get_numa_info
+
+#
 # x32-specific system call numbers start at 512 to avoid cache impact
 # for native 64-bit operation.
 #
diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile
index c366c0a..4a7bb2e 100644
--- a/arch/x86/entry/vdso/Makefile
+++ b/arch/x86/entry/vdso/Makefile
@@ -17,7 +17,7 @@ VDSO32-$(CONFIG_X86_32)		:= y
 VDSO32-$(CONFIG_IA32_EMULATION)	:= y

 # files to link into the vdso
-vobjs-y := vdso-note.o vclock_gettime.o vgetcpu.o
+vobjs-y := vdso-note.o vclock_gettime.o vgetcpu.o vnuma_topology.o

 # files to link into kernel
 obj-y				+= vma.o
@@ -85,6 +85,7 @@ CFLAGS_REMOVE_vdso-note.o = -pg
 CFLAGS_REMOVE_vclock_gettime.o = -pg
 CFLAGS_REMOVE_vgetcpu.o = -pg
 CFLAGS_REMOVE_vvar.o = -pg
+CFLAGS_REMOVE_vnuma_topology.o = -pg

 #
 # X32 processes use x32 vDSO to access 64bit kernel data.
diff --git a/arch/x86/entry/vdso/vdso.lds.S b/arch/x86/entry/vdso/vdso.lds.S
index d3a2dce..e010506 100644
--- a/arch/x86/entry/vdso/vdso.lds.S
+++ b/arch/x86/entry/vdso/vdso.lds.S
@@ -25,6 +25,14 @@ VERSION {
 		__vdso_getcpu;
 		time;
 		__vdso_time;
+		get_topology_version;
+		__vdso_get_topology_version;
+		node_to_cpus;
+		__vdso_node_to_cpus;
+		getmemnodemask;
+		__vdso_getmemnodemask;
+		getnumanode;
+		__vdso_getnumanode;
 	local: *;
 	};
 }
diff --git a/arch/x86/entry/vdso/vnuma_topology.c b/arch/x86/entry/vdso/vnuma_topology.c
new file mode 100644
index 0000000..e1e4a7c
--- /dev/null
+++ b/arch/x86/entry/vdso/vnuma_topology.c
@@ -0,0 +1,79 @@
+#include <linux/kernel.h>
+#include <linux/getcpu.h>
+#include <asm/vgtod.h>
+#include <asm/vvar.h>
+
+#define sdata (&VVAR(vsyscall_gtod_data))
+
+notrace unsigned long
+__vdso_get_topology_version(void)
+{
+	unsigned long seq, version = 0;
+
+	do {
+		seq = gtod_read_begin(sdata);
+		version = sdata->topology_version;
+	} while (unlikely(gtod_read_retry(sdata, seq)));
+
+	return version;
+}
+
+unsigned long get_topology_version(void)
+	__attribute__((weak, alias("__vdso_get_topology_version")));
+
+notrace unsigned long
+__vdso_node_to_cpus(int node)
+{
+	int i;
+	unsigned long seq, mask = 0;
+
+	do {
+		seq = gtod_read_begin(sdata);
+		for (i = 0; i < sdata->ncpus; i++)
+			if (sdata->cpu_to_node[i] == node)
+				mask |= (1UL << i);
+	} while (unlikely(gtod_read_retry(sdata, seq)));
+
+	return mask;
+}
+
+unsigned long node_to_cpus(int node)
+	__attribute__((weak, alias("__vdso_node_to_cpus")));
+
+notrace unsigned long
+__vdso_getmemnodemask(void)
+{
+	int nid;
+	unsigned long seq, mask = 0;
+
+	do {
+		seq = gtod_read_begin(sdata);
+		for (nid = 0; nid < sdata->numa_num_nodes; nid++)
+			if (sdata->memnode_map[nid])
+				mask |= (1UL << nid);
+	} while (unlikely(gtod_read_retry(sdata, seq)));
+
+	return mask;
+}
+
+unsigned long getmemnodemask(void)
+	__attribute__((weak, alias("__vdso_getmemnodemask")));
+
+notrace long
+__vdso_getnumanode(int *node)
+{
+	unsigned long seq;
+	unsigned cpu;
+
+        cpu = __getcpu() & VGETCPU_CPU_MASK;
+
+	do {
+		seq = gtod_read_begin(sdata);
+		*node = sdata->cpu_to_node[cpu];
+	} while (unlikely(gtod_read_retry(sdata, seq)));
+
+        return 0;
+}
+
+long getnumanode(int *node)
+        __attribute__((weak, alias("__vdso_getnumanode")));
diff --git a/arch/x86/entry/vsyscall/vsyscall_gtod.c b/arch/x86/entry/vsyscall/vsyscall_gtod.c
index e1216dd..2cf3c96 100644
--- a/arch/x86/entry/vsyscall/vsyscall_gtod.c
+++ b/arch/x86/entry/vsyscall/vsyscall_gtod.c
@@ -32,6 +32,9 @@ void update_vsyscall(struct timekeeper *tk)
 	int vclock_mode = tk->tkr_mono.clock->archdata.vclock_mode;
 	struct vsyscall_gtod_data *vdata = &vsyscall_gtod_data;

+	unsigned long old_topology_version;
+	int nid, cid;
+
 	/* Mark the new vclock used. */
 	BUILD_BUG_ON(VCLOCK_MAX >= 32);
 	WRITE_ONCE(vclocks_used, READ_ONCE(vclocks_used) | (1 << vclock_mode));
@@ -74,5 +77,18 @@ void update_vsyscall(struct timekeeper *tk)
 		vdata->monotonic_time_coarse_sec++;
 	}

+	/* copy vNUMA data */
+	vdata->ncpus = cid = xen_num_cpus;
+	vdata->numa_num_nodes = nid = xen_numa_num_nodes;
+
+	old_topology_version = vdata->topology_version;
+	vdata->topology_version = atomic64_read(&topology_version);
+	if (old_topology_version != vdata->topology_version) {
+		while (cid--)
+			vdata->cpu_to_node[cid] = xen_cpu_to_node(cid);
+		while (nid--)
+			vdata->memnode_map[nid] = xen_memnode_map[nid];
+	}
+
 	gtod_write_end(vdata);
 }
diff --git a/arch/x86/include/asm/vgtod.h b/arch/x86/include/asm/vgtod.h
index 5225068..6e37698 100644
--- a/arch/x86/include/asm/vgtod.h
+++ b/arch/x86/include/asm/vgtod.h
@@ -5,6 +5,9 @@
 #include <linux/compiler.h>
 #include <linux/clocksource.h>

+#include <asm/xen/vnuma.h>
+#include <xen/xen.h>
+
 #ifdef BUILD_VDSO32_64
 typedef u64 gtod_long_t;
 #else
@@ -35,6 +38,15 @@ struct vsyscall_gtod_data {

 	int		tz_minuteswest;
 	int		tz_dsttime;
+
+	/* vNUMA topology data */
+	gtod_long_t	topology_version;
+
+	int		ncpus;
+	int		cpu_to_node[MAX_VIRT_CPUS];
+
+	int		numa_num_nodes;
+	int		memnode_map[XEN_NUMNODES];
 };
 extern struct vsyscall_gtod_data vsyscall_gtod_data;

diff --git a/arch/x86/include/asm/xen/page.h b/arch/x86/include/asm/xen/page.h
index c6b8424..0f7136b 100644
--- a/arch/x86/include/asm/xen/page.h
+++ b/arch/x86/include/asm/xen/page.h
@@ -12,6 +12,7 @@
 #include <linux/uaccess.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
+#include <asm/xen/vnuma.h>

 #include <xen/interface/xen.h>
 #include <xen/interface/grant_table.h>
@@ -229,6 +230,25 @@ static inline unsigned long gfn_to_pfn(unsigned long gfn)
 		return mfn_to_pfn(gfn);
 }

+static inline int xen_pfn_to_nid(unsigned long gfn)
+{
+	unsigned long mfn;
+	int mr_id, pnode;
+
+	mfn = xen_p2m_addr[gfn];
+
+	for (pnode = 0; pnode < xen_numa_num_nodes; pnode++) {
+		mr_id = pnode * 2;
+		if ((mfn - xen_numa_memranges[mr_id]) <
+				xen_numa_memranges[mr_id+1])
+			return pnode;
+	}
+
+	return NUMA_NO_NODE;
+}
+
+#define xen_page_to_nid(page)	xen_pfn_to_nid(page_to_pfn(page))
+
 /* Pseudo-physical <-> Bus conversion */
 #define pfn_to_bfn(pfn)		pfn_to_gfn(pfn)
 #define bfn_to_pfn(bfn)		gfn_to_pfn(bfn)
diff --git a/arch/x86/include/asm/xen/vnuma.h b/arch/x86/include/asm/xen/vnuma.h
new file mode 100644
index 0000000..260ccd9
--- /dev/null
+++ b/arch/x86/include/asm/xen/vnuma.h
@@ -0,0 +1,35 @@
+#ifndef _ASM_X86_VNUMA_H
+#define _ASM_X86_VNUMA_H
+
+#include <xen/xen.h>
+
+#define XEN_NUMNODES	16
+
+#define xen_cpu_to_node(cpu)	HYPERVISOR_shared_info->vcpu_to_pnode[cpu]
+
+/* Physical/static numa information */
+extern int xen_numa_distance[];
+extern unsigned long xen_numa_memranges[];
+extern int xen_numa_num_nodes;
+extern int nodelists[][XEN_NUMNODES];
+
+/* Virtual/dynamic numa information */
+extern int xen_num_cpus;
+extern int xen_memnode_map[];
+extern atomic64_t topology_version;
+
+static inline unsigned long xen_cpumask_of_node(int node)
+{
+	int cpu;
+	unsigned long mask = 0;
+
+	for (cpu = 0; cpu < xen_num_cpus; cpu++)
+		if (xen_cpu_to_node(cpu) == node)
+			mask |= (1UL << cpu);
+	return mask;
+}
+
+int xen_numa_init(void);
+void xen_update_vcpu_to_pnode(void);
+
+#endif /* _ASM_X86_VNUMA_H */
diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c
index 25504d5..f45046f 100644
--- a/arch/x86/mm/numa.c
+++ b/arch/x86/mm/numa.c
@@ -17,6 +17,7 @@
 #include <asm/dma.h>
 #include <asm/amd_nb.h>

+#include "asm/xen/vnuma.h"
 #include "numa_internal.h"

 int numa_off;
@@ -709,6 +710,8 @@ static int __init dummy_numa_init(void)
 void __init x86_numa_init(void)
 {
 	if (!numa_off) {
+		if (!numa_init(xen_numa_init))
+			return;
 #ifdef CONFIG_ACPI_NUMA
 		if (!numa_init(x86_acpi_numa_init))
 			return;
diff --git a/arch/x86/xen/Makefile b/arch/x86/xen/Makefile
index d83cb54..da38916 100644
--- a/arch/x86/xen/Makefile
+++ b/arch/x86/xen/Makefile
@@ -34,3 +34,4 @@ obj-$(CONFIG_XEN_DOM0)		+= vga.o
 obj-$(CONFIG_SWIOTLB_XEN)	+= pci-swiotlb-xen.o
 obj-$(CONFIG_XEN_EFI)		+= efi.o
 obj-$(CONFIG_XEN_PVH)	 	+= xen-pvh.o
+obj-$(CONFIG_NUMA)		+= vnuma.o
diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c
index c114ca7..2665a76 100644
--- a/arch/x86/xen/setup.c
+++ b/arch/x86/xen/setup.c
@@ -21,6 +21,7 @@
 #include <asm/numa.h>
 #include <asm/xen/hypervisor.h>
 #include <asm/xen/hypercall.h>
+#include <asm/xen/vnuma.h>

 #include <xen/xen.h>
 #include <xen/page.h>
@@ -1042,6 +1043,9 @@ void __init xen_arch_setup(void)
 	WARN_ON(xen_set_default_idle());
 	fiddle_vdso();
 #ifdef CONFIG_NUMA
-	numa_off = 1;
+	if (xen_initial_domain())
+		numa_off = 1;
+	else
+		numa_off = 0;
 #endif
 }
diff --git a/arch/x86/xen/smp.c b/arch/x86/xen/smp.c
index 77c959c..59f0e4a 100644
--- a/arch/x86/xen/smp.c
+++ b/arch/x86/xen/smp.c
@@ -15,6 +15,7 @@
 static DEFINE_PER_CPU(struct xen_common_irq, xen_callfunc_irq) = { .irq = -1 };
 static DEFINE_PER_CPU(struct xen_common_irq, xen_callfuncsingle_irq) = { .irq = -1 };
 static DEFINE_PER_CPU(struct xen_common_irq, xen_debug_irq) = { .irq = -1 };
+static DEFINE_PER_CPU(struct xen_common_irq, xen_topology_irq) = { .irq = -1 };

 static irqreturn_t xen_call_function_interrupt(int irq, void *dev_id);
 static irqreturn_t xen_call_function_single_interrupt(int irq, void *dev_id);
@@ -50,6 +51,12 @@ void xen_smp_intr_free(unsigned int cpu)
 		kfree(per_cpu(xen_debug_irq, cpu).name);
 		per_cpu(xen_debug_irq, cpu).name = NULL;
 	}
+	if (per_cpu(xen_topology_irq, cpu).irq >= 0) {
+		unbind_from_irqhandler(per_cpu(xen_topology_irq, cpu).irq, NULL);
+		per_cpu(xen_topology_irq, cpu).irq = -1;
+		kfree(per_cpu(xen_topology_irq, cpu).name);
+		per_cpu(xen_topology_irq, cpu).name = NULL;
+	}
 	if (per_cpu(xen_callfuncsingle_irq, cpu).irq >= 0) {
 		unbind_from_irqhandler(per_cpu(xen_callfuncsingle_irq, cpu).irq,
 				       NULL);
@@ -62,7 +69,7 @@ void xen_smp_intr_free(unsigned int cpu)
 int xen_smp_intr_init(unsigned int cpu)
 {
 	int rc;
-	char *resched_name, *callfunc_name, *debug_name;
+	char *resched_name, *callfunc_name, *debug_name, *topology_name;

 	resched_name = kasprintf(GFP_KERNEL, "resched%d", cpu);
 	rc = bind_ipi_to_irqhandler(XEN_RESCHEDULE_VECTOR,
@@ -97,6 +104,15 @@ int xen_smp_intr_init(unsigned int cpu)
 	per_cpu(xen_debug_irq, cpu).irq = rc;
 	per_cpu(xen_debug_irq, cpu).name = debug_name;

+	topology_name = kasprintf(GFP_KERNEL, "topology%d", cpu);
+	rc = bind_virq_to_irqhandler(VIRQ_TOPOLOGY, cpu, xen_topology_interrupt,
+				     IRQF_PERCPU | IRQF_NOBALANCING,
+				     topology_name, NULL);
+	if (rc < 0)
+		goto fail;
+	per_cpu(xen_topology_irq, cpu).irq = rc;
+	per_cpu(xen_topology_irq, cpu).name = topology_name;
+
 	callfunc_name = kasprintf(GFP_KERNEL, "callfuncsingle%d", cpu);
 	rc = bind_ipi_to_irqhandler(XEN_CALL_FUNCTION_SINGLE_VECTOR,
 				    cpu,
diff --git a/arch/x86/xen/vnuma.c b/arch/x86/xen/vnuma.c
new file mode 100644
index 0000000..a01f60c
--- /dev/null
+++ b/arch/x86/xen/vnuma.c
@@ -0,0 +1,221 @@
+#include <linux/err.h>
+#include <linux/memblock.h>
+#include <xen/interface/xen.h>
+#include <xen/interface/memory.h>
+#include <asm/xen/interface.h>
+#include <asm/xen/hypercall.h>
+#include <asm/xen/vnuma.h>
+#include <linux/syscalls.h>
+
+int xen_numa_distance[XEN_NUMNODES*XEN_NUMNODES];
+unsigned long xen_numa_memranges[2*XEN_NUMNODES];
+int xen_numa_num_nodes;
+int nodelists[XEN_NUMNODES][XEN_NUMNODES];
+
+int xen_num_cpus;
+int xen_memnode_map[XEN_NUMNODES];
+atomic64_t topology_version;
+
+#define NUMA_NODEMASK_SZ	0
+#define NUMA_MAXNODE		1
+#define NUMA_DISTANCE		2
+
+SYSCALL_DEFINE2(get_numa_info, int, numa_info, void __user *, buff)
+{
+	int nid1, nid2;
+	int *dist_table;
+
+	switch(numa_info) {
+		case NUMA_NODEMASK_SZ:
+			*(int *)buff = MAX_NUMNODES;
+			break;
+		case NUMA_MAXNODE:
+			*(int *)buff = xen_numa_num_nodes - 1;
+			break;
+		case NUMA_DISTANCE:
+			dist_table = (int *)buff;
+			for (nid1 = 0; nid1 < xen_numa_num_nodes; nid1++) {
+				for (nid2 = 0; nid2 < xen_numa_num_nodes; nid2++) {
+					dist_table[nid1*XEN_NUMNODES+nid2] =
+						xen_numa_distance[nid1*XEN_NUMNODES+nid2];
+				}
+			}
+			break;
+	}
+	return 0;
+}
+
+void xen_update_vcpu_to_pnode(void)
+{
+	int i, cpu;
+	int pnode, this_pnode;
+
+	cpu = smp_processor_id();
+	pnode = xen_cpu_to_node(cpu);
+
+	if (!xen_memnode_map[pnode]) {
+		/*
+		 * The system doesn't have memory residing in the new node.
+		 * We try to find the closest memory node.
+		 */
+		i = xen_numa_num_nodes - 2;
+		do {
+			this_pnode = nodelists[pnode][i];
+			if (xen_memnode_map[this_pnode]) {
+				xen_cpu_to_node(cpu) = this_pnode;
+				break;
+			}
+		} while (i--);
+	}
+}
+
+/*
+ * Called from numa_init if numa_off = 0.
+ */
+int __init xen_numa_init(void)
+{
+	int i, j;
+	struct xen_numa_topology_info numa_topo;
+
+	set_xen_guest_handle(numa_topo.memranges, xen_numa_memranges);
+	set_xen_guest_handle(numa_topo.distance, xen_numa_distance);
+	set_xen_guest_handle(numa_topo.memnode_map, xen_memnode_map);
+
+	if (HYPERVISOR_memory_op(XENMEM_get_numainfo, &numa_topo) < 0)
+		goto out;
+
+	xen_numa_num_nodes = numa_topo.nr_nodes;
+	xen_num_cpus = numa_topo.nr_vcpus;
+
+	for (i = 0; i < xen_numa_num_nodes; i++) {
+		printk(KERN_INFO "Node %d's memory range: start %lu num %lu\n",
+			i, xen_numa_memranges[2*i], xen_numa_memranges[2*i+1]);
+
+		for (j = 0; j < xen_numa_num_nodes; j++)
+			printk(KERN_INFO "Distance(%d, %d) = %d\n",
+				i, j, xen_numa_distance[i*XEN_NUMNODES+j]);
+	}
+
+	atomic64_set(&topology_version, 0);
+
+	/*
+	 * We have enough information to properly build the nodelists
+	 * but I'm just too lazy at this point. So they are hardcoded.
+	 * It's not very important anyways.
+	 */
+	/*
+	nodelists[0][0] = 7;
+	nodelists[0][1] = 5;
+	nodelists[0][2] = 3;
+	nodelists[0][3] = 6;
+	nodelists[0][4] = 4;
+	nodelists[0][5] = 2;
+	nodelists[0][6] = 1;
+	nodelists[0][7] = 0;
+
+	nodelists[1][0] = 6;
+	nodelists[1][1] = 5;
+	nodelists[1][2] = 2;
+	nodelists[1][3] = 7;
+	nodelists[1][4] = 4;
+	nodelists[1][5] = 3;
+	nodelists[1][6] = 0;
+	nodelists[1][7] = 1;
+
+	nodelists[2][0] = 1;
+	nodelists[2][1] = 7;
+	nodelists[2][2] = 6;
+	nodelists[2][3] = 5;
+	nodelists[2][4] = 4;
+	nodelists[2][5] = 3;
+	nodelists[2][6] = 0;
+	nodelists[2][7] = 2;
+
+	nodelists[3][0] = 7;
+	nodelists[3][1] = 6;
+	nodelists[3][2] = 0;
+	nodelists[3][3] = 5;
+	nodelists[3][4] = 4;
+	nodelists[3][5] = 2;
+	nodelists[3][6] = 1;
+	nodelists[3][7] = 3;
+	*/
+
+	nodelists[0][0] = 7;
+	nodelists[0][1] = 5;
+	nodelists[0][2] = 6;
+	nodelists[0][3] = 4;
+	nodelists[0][4] = 3;
+	nodelists[0][5] = 2;
+	nodelists[0][6] = 1;
+	nodelists[0][7] = 0;
+
+	nodelists[1][0] = 6;
+	nodelists[1][1] = 5;
+	nodelists[1][2] = 7;
+	nodelists[1][3] = 4;
+	nodelists[1][4] = 2;
+	nodelists[1][5] = 3;
+	nodelists[1][6] = 0;
+	nodelists[1][7] = 1;
+
+	nodelists[2][0] = 7;
+	nodelists[2][1] = 6;
+	nodelists[2][2] = 5;
+	nodelists[2][3] = 4;
+	nodelists[2][4] = 1;
+	nodelists[2][5] = 3;
+	nodelists[2][6] = 0;
+	nodelists[2][7] = 2;
+
+	nodelists[3][0] = 7;
+	nodelists[3][1] = 6;
+	nodelists[3][2] = 5;
+	nodelists[3][3] = 4;
+	nodelists[3][4] = 0;
+	nodelists[3][5] = 2;
+	nodelists[3][6] = 1;
+	nodelists[3][7] = 3;
+
+	nodelists[4][0] = 7;
+	nodelists[4][1] = 6;
+	nodelists[4][2] = 5;
+	nodelists[4][3] = 3;
+	nodelists[4][4] = 2;
+	nodelists[4][5] = 1;
+	nodelists[4][6] = 0;
+	nodelists[4][7] = 4;
+
+	nodelists[5][0] = 6;
+	nodelists[5][1] = 1;
+	nodelists[5][2] = 0;
+	nodelists[5][3] = 7;
+	nodelists[5][4] = 4;
+	nodelists[5][5] = 3;
+	nodelists[5][6] = 2;
+	nodelists[5][7] = 5;
+
+	nodelists[6][0] = 5;
+	nodelists[6][1] = 3;
+	nodelists[6][2] = 1;
+	nodelists[6][3] = 7;
+	nodelists[6][4] = 4;
+	nodelists[6][5] = 2;
+	nodelists[6][6] = 0;
+	nodelists[6][7] = 6;
+
+	nodelists[7][0] = 4;
+	nodelists[7][1] = 3;
+	nodelists[7][2] = 0;
+	nodelists[7][3] = 6;
+	nodelists[7][4] = 5;
+	nodelists[7][5] = 2;
+	nodelists[7][6] = 1;
+	nodelists[7][7] = 7;
+
+out:
+	node_set(0, numa_nodes_parsed);
+	numa_add_memblk(0, 0, PFN_PHYS(max_pfn));
+
+	return 0;
+}
diff --git a/arch/x86/xen/xen-ops.h b/arch/x86/xen/xen-ops.h
index f377e18..b5ba220 100644
--- a/arch/x86/xen/xen-ops.h
+++ b/arch/x86/xen/xen-ops.h
@@ -74,6 +74,7 @@
 void __init xen_hvm_init_time_ops(void);

 irqreturn_t xen_debug_interrupt(int irq, void *dev_id);
+irqreturn_t xen_topology_interrupt(int irq, void *dev_id);

 bool xen_vcpu_stolen(int vcpu);

diff --git a/drivers/xen/events/events_2l.c b/drivers/xen/events/events_2l.c
index 8edef51..e64852f 100644
--- a/drivers/xen/events/events_2l.c
+++ b/drivers/xen/events/events_2l.c
@@ -14,6 +14,7 @@
 #include <asm/sync_bitops.h>
 #include <asm/xen/hypercall.h>
 #include <asm/xen/hypervisor.h>
+#include <asm/xen/vnuma.h>

 #include <xen/xen.h>
 #include <xen/xen-ops.h>
@@ -346,6 +347,14 @@ irqreturn_t xen_debug_interrupt(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }

+irqreturn_t xen_topology_interrupt(int irq, void *dev_id)
+{
+	atomic64_inc(&topology_version);
+
+	xen_update_vcpu_to_pnode();
+	return IRQ_HANDLED;
+}
+
 static void evtchn_2l_resume(void)
 {
 	int i;
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 7101437..04a84d7 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -462,12 +462,13 @@ static inline void arch_alloc_page(struct page *page, int order) { }

 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
-							nodemask_t *nodemask);
+				nodemask_t *nodemask, int pv_preferred_nid);

 static inline struct page *
 __alloc_pages(gfp_t gfp_mask, unsigned int order, int preferred_nid)
 {
-	return __alloc_pages_nodemask(gfp_mask, order, preferred_nid, NULL);
+	return __alloc_pages_nodemask(gfp_mask, order,
+			preferred_nid, NULL, NUMA_NO_NODE);
 }

 /*
diff --git a/include/linux/mempolicy.h b/include/linux/mempolicy.h
index 5228c62..2fb904c 100644
--- a/include/linux/mempolicy.h
+++ b/include/linux/mempolicy.h
@@ -44,10 +44,12 @@
  */
 struct mempolicy {
 	atomic_t refcnt;
-	unsigned short mode; 	/* See MPOL_* above */
+	unsigned short mode;	/* See MPOL_* above */
 	unsigned short flags;	/* See set_mempolicy() MPOL_F_* above */
+	short pv_preferred_node; /* preferred/bind */
+	nodemask_t pv_nodes;     /* interleave */
 	union {
-		short 		 preferred_node; /* preferred */
+		short		 preferred_node; /* preferred */
 		nodemask_t	 nodes;		/* interleave/bind */
 		/* undefined for default */
 	} v;
@@ -202,7 +204,7 @@ static inline bool vma_migratable(struct vm_area_struct *vma)
 	return true;
 }

-extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long);
+extern int mpol_misplaced(struct page *, struct vm_area_struct *, unsigned long, int *);
 extern void mpol_put_task_policy(struct task_struct *);

 #else
@@ -300,7 +302,7 @@ static inline int mpol_parse_str(char *str, struct mempolicy **mpol)
 #endif

 static inline int mpol_misplaced(struct page *page, struct vm_area_struct *vma,
-				 unsigned long address)
+				 unsigned long address, int *xpv_polnid)
 {
 	return -1; /* no node preference */
 }
diff --git a/include/linux/migrate.h b/include/linux/migrate.h
index 895ec0c..04596ca 100644
--- a/include/linux/migrate.h
+++ b/include/linux/migrate.h
@@ -52,7 +52,7 @@ static inline struct page *new_page_nodemask(struct page *page,
 		gfp_mask |= __GFP_HIGHMEM;

 	new_page = __alloc_pages_nodemask(gfp_mask, order,
-				preferred_nid, nodemask);
+			preferred_nid, nodemask, NUMA_NO_NODE);

 	if (new_page && PageTransHuge(page))
 		prep_transhuge_page(new_page);
@@ -127,14 +127,16 @@ static inline void __ClearPageMovable(struct page *page)
 #ifdef CONFIG_NUMA_BALANCING
 extern bool pmd_trans_migrating(pmd_t pmd);
 extern int migrate_misplaced_page(struct page *page,
-				  struct vm_area_struct *vma, int node);
+				  struct vm_area_struct *vma,
+				  int node, int xpv_node);
 #else
 static inline bool pmd_trans_migrating(pmd_t pmd)
 {
 	return false;
 }
 static inline int migrate_misplaced_page(struct page *page,
-					 struct vm_area_struct *vma, int node)
+					 struct vm_area_struct *vma,
+					 int node, int xpv_node)
 {
 	return -EAGAIN; /* can't migrate now */
 }
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index c85f11d..2ca2d2d 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -218,6 +218,7 @@ struct page {
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
+	short machine_nid;
 }
 /*
  * The struct page can be forced to be double word aligned so that atomic ops
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index c9c4a81..19a3f69 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -19,6 +19,7 @@
 #include <linux/page-flags-layout.h>
 #include <linux/atomic.h>
 #include <asm/page.h>
+#include <asm/xen/vnuma.h>

 /* Free memory management - zoned buddy allocator.  */
 #ifndef CONFIG_FORCE_MAX_ZONEORDER
@@ -281,6 +282,7 @@ struct per_cpu_pages {

 struct per_cpu_pageset {
 	struct per_cpu_pages pcp;
+	struct per_cpu_pages numa_pcp[XEN_NUMNODES];
 #ifdef CONFIG_NUMA
 	s8 expire;
 	u16 vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS];
@@ -459,6 +461,7 @@ struct zone {

 	/* free areas of different sizes */
 	struct free_area	free_area[MAX_ORDER];
+	struct free_area	numa_free_area[XEN_NUMNODES][MAX_ORDER];

 	/* zone flags, see below */
 	unsigned long		flags;
diff --git a/include/linux/sched.h b/include/linux/sched.h
index fdf74f2..16b5a5f 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -932,6 +932,7 @@ struct task_struct {
 	/* Protected by alloc_lock: */
 	struct mempolicy		*mempolicy;
 	short				il_prev;
+	short				pv_il_prev;
 	short				pref_node_fork;
 #endif
 #ifdef CONFIG_NUMA_BALANCING
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index a78186d..67784f0 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -781,6 +781,7 @@ asmlinkage long sys_get_mempolicy(int __user *policy,
 				unsigned long __user *nmask,
 				unsigned long maxnode,
 				unsigned long addr, unsigned long flags);
+asmlinkage long sys_get_numa_info(int numa_info, void __user *buff);

 asmlinkage long sys_inotify_init(void);
 asmlinkage long sys_inotify_init1(int flags);
diff --git a/include/xen/interface/memory.h b/include/xen/interface/memory.h
index 583dd93..6b68629 100644
--- a/include/xen/interface/memory.h
+++ b/include/xen/interface/memory.h
@@ -265,4 +265,69 @@ struct xen_remove_from_physmap {
 };
 DEFINE_GUEST_HANDLE_STRUCT(xen_remove_from_physmap);

+/*
+ * XENMEM_get_vnumainfo used by guest to get
+ * vNUMA topology from hypervisor.
+ */
+#define XENMEM_get_vnumainfo                26
+
+/* vNUMA node memory ranges */
+struct xen_vmemrange {
+    uint64_t start, end;
+    unsigned int flags;
+    unsigned int nid;
+};
+typedef struct xen_vmemrange xen_vmemrange_t;
+DEFINE_GUEST_HANDLE(xen_vmemrange_t);
+
+/*
+ * vNUMA topology specifies vNUMA node number, distance table,
+ * memory ranges and vcpu mapping provided for guests.
+ * XENMEM_get_vnumainfo hypercall expects to see from guest
+ * nr_vnodes, nr_vmemranges and nr_vcpus to indicate available memory.
+ * After filling guests structures, nr_vnodes, nr_vmemranges and nr_vcpus
+ * copied back to guest. Domain returns expected values of nr_vnodes,
+ * nr_vmemranges and nr_vcpus to guest if the values where incorrect.
+ */
+struct xen_vnuma_topology_info {
+    /* IN */
+    domid_t domid;
+    uint16_t pad;
+    /* IN/OUT */
+    unsigned int nr_vnodes;
+    unsigned int nr_vcpus;
+    unsigned int nr_vmemranges;
+    /* OUT */
+    union {
+	GUEST_HANDLE(uint) h;
+	uint64_t pad;
+    } vdistance;
+    union {
+	GUEST_HANDLE(uint) h;
+	uint64_t pad;
+    } vcpu_to_vnode;
+    union {
+	GUEST_HANDLE(xen_vmemrange_t) h;
+	uint64_t pad;
+    } vmemrange;
+};
+typedef struct xen_vnuma_topology_info xen_vnuma_topology_info_t;
+DEFINE_GUEST_HANDLE(xen_vnuma_topology_info_t);
+
+struct xen_numa_topology_info {
+	/* Machine/physical information */
+	unsigned int nr_nodes;
+	GUEST_HANDLE(xen_ulong_t) memranges;
+	GUEST_HANDLE(uint) distance;
+
+	/* VM information */
+	unsigned int nr_vcpus;
+	GUEST_HANDLE(uint) memnode_map;
+};
+
+typedef struct xen_numa_topology_info xen_numa_topology_info_t;
+DEFINE_GUEST_HANDLE(xen_numa_topology_info_t);
+
+#define XENMEM_get_numainfo                28
+
 #endif /* __XEN_PUBLIC_MEMORY_H__ */
diff --git a/include/xen/interface/xen.h b/include/xen/interface/xen.h
index 4f4830e..3630a1b 100644
--- a/include/xen/interface/xen.h
+++ b/include/xen/interface/xen.h
@@ -115,6 +115,7 @@
 #define VIRQ_XC_RESERVED 11 /* G. Reserved for XenClient                     */
 #define VIRQ_ENOMEM     12 /* G. (DOM0) Low on heap memory       */
 #define VIRQ_XENPMU     13  /* PMC interrupt                                 */
+#define VIRQ_TOPOLOGY   14  /* V. Topology has been changed.                 */

 /* Architecture-specific VIRQ definitions. */
 #define VIRQ_ARCH_0    16
@@ -600,6 +601,7 @@ struct shared_info {

 	struct arch_shared_info arch;

+	uint8_t vcpu_to_pnode[MAX_VIRT_CPUS];
 };

 /*
diff --git a/include/xen/xen.h b/include/xen/xen.h
index 9d4340c..9ba91b8 100644
--- a/include/xen/xen.h
+++ b/include/xen/xen.h
@@ -22,6 +22,8 @@ enum xen_domain_type {

 #define xen_domain()		(xen_domain_type != XEN_NATIVE)
 #define xen_pv_domain()		(xen_domain_type == XEN_PV_DOMAIN)
+#define xen_pv_domu()		(xen_pv_domain() && \
+					!(xen_start_info->flags & SIF_INITDOMAIN))
 #define xen_hvm_domain()	(xen_domain_type == XEN_HVM_DOMAIN)
 #define xen_pvh_domain()	(xen_pvh)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 5c09ddf..6934d06 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -36,6 +36,9 @@

 #include <trace/events/sched.h>

+#include <asm/xen/vnuma.h>
+#include <xen/xen.h>
+
 #include "sched.h"

 /*
@@ -1165,13 +1168,13 @@ static unsigned int task_scan_max(struct task_struct *p)
 static void account_numa_enqueue(struct rq *rq, struct task_struct *p)
 {
 	rq->nr_numa_running += (p->numa_preferred_nid != -1);
-	rq->nr_preferred_running += (p->numa_preferred_nid == task_node(p));
+	rq->nr_preferred_running += (p->numa_preferred_nid == xen_cpu_to_node(task_cpu(p)));
 }

 static void account_numa_dequeue(struct rq *rq, struct task_struct *p)
 {
 	rq->nr_numa_running -= (p->numa_preferred_nid != -1);
-	rq->nr_preferred_running -= (p->numa_preferred_nid == task_node(p));
+	rq->nr_preferred_running -= (p->numa_preferred_nid == xen_cpu_to_node(task_cpu(p)));
 }

 /* Shared or private faults. */
@@ -1196,7 +1199,7 @@ pid_t task_numa_group_id(struct task_struct *p)
  */
 static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv)
 {
-	return NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv;
+	return NR_NUMA_HINT_FAULT_TYPES * (s * xen_numa_num_nodes + nid) + priv;
 }

 static inline unsigned long task_faults(struct task_struct *p, int nid)
@@ -1228,8 +1231,9 @@ static inline unsigned long group_faults_priv(struct numa_group *ng)
 	unsigned long faults = 0;
 	int node;

-	for_each_online_node(node) {
-		faults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];
+	for (node = 0; node < xen_numa_num_nodes; node++) {
+		if (xen_memnode_map[node])
+			faults += ng->faults[task_faults_idx(NUMA_MEM, node, 1)];
 	}

 	return faults;
@@ -1240,8 +1244,9 @@ static inline unsigned long group_faults_shared(struct numa_group *ng)
 	unsigned long faults = 0;
 	int node;

-	for_each_online_node(node) {
-		faults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];
+	for (node = 0; node < xen_numa_num_nodes; node++) {
+		if (xen_memnode_map[node])
+			faults += ng->faults[task_faults_idx(NUMA_MEM, node, 0)];
 	}

 	return faults;
@@ -1344,7 +1349,7 @@ static inline unsigned long task_weight(struct task_struct *p, int nid,
 		return 0;

 	faults = task_faults(p, nid);
-	faults += score_nearby_nodes(p, nid, dist, true);
+	/* faults += score_nearby_nodes(p, nid, dist, true); */

 	return 1000 * faults / total_faults;
 }
@@ -1363,7 +1368,7 @@ static inline unsigned long group_weight(struct task_struct *p, int nid,
 		return 0;

 	faults = group_faults(p, nid);
-	faults += score_nearby_nodes(p, nid, dist, false);
+	/* faults += score_nearby_nodes(p, nid, dist, false); */

 	return 1000 * faults / total_faults;
 }
@@ -1372,7 +1377,7 @@ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
 				int src_nid, int dst_cpu)
 {
 	struct numa_group *ng = p->numa_group;
-	int dst_nid = cpu_to_node(dst_cpu);
+	int dst_nid = xen_cpu_to_node(dst_cpu);
 	int last_cpupid, this_cpupid;

 	this_cpupid = cpu_pid_to_cpupid(dst_cpu, current->pid);
@@ -1396,7 +1401,7 @@ bool should_numa_migrate_memory(struct task_struct *p, struct page * page,
 	 */
 	last_cpupid = page_cpupid_xchg_last(page, this_cpupid);
 	if (!cpupid_pid_unset(last_cpupid) &&
-				cpupid_to_nid(last_cpupid) != dst_nid)
+				xen_cpu_to_node(cpupid_to_cpu(last_cpupid)) != dst_nid)
 		return false;

 	/* Always allow migrate on private faults */
@@ -1451,10 +1456,10 @@ struct numa_stats {
 static void update_numa_stats(struct numa_stats *ns, int nid)
 {
 	int smt, cpu, cpus = 0;
-	unsigned long capacity;
+	unsigned long capacity, mask = xen_cpumask_of_node(nid);

 	memset(ns, 0, sizeof(*ns));
-	for_each_cpu(cpu, cpumask_of_node(nid)) {
+	for_each_cpu(cpu, (struct cpumask *)&mask) {
 		struct rq *rq = cpu_rq(cpu);

 		ns->nr_running += rq->nr_running;
@@ -1703,8 +1708,9 @@ static void task_numa_find_cpu(struct task_numa_env *env,
 				long taskimp, long groupimp)
 {
 	int cpu;
+	unsigned long mask = xen_cpumask_of_node(env->dst_nid);

-	for_each_cpu(cpu, cpumask_of_node(env->dst_nid)) {
+	for_each_cpu(cpu, (struct cpumask *)&mask) {
 		/* Skip this CPU if the source task cannot migrate */
 		if (!cpumask_test_cpu(cpu, &env->p->cpus_allowed))
 			continue;
@@ -1745,7 +1751,7 @@ static int task_numa_migrate(struct task_struct *p)
 		.p = p,

 		.src_cpu = task_cpu(p),
-		.src_nid = task_node(p),
+		.src_nid = xen_cpu_to_node(task_cpu(p)),

 		.imbalance_pct = 112,

@@ -1779,12 +1785,12 @@ static int task_numa_migrate(struct task_struct *p)
 	 * elsewhere, so there is no point in (re)trying.
 	 */
 	if (unlikely(!sd)) {
-		p->numa_preferred_nid = task_node(p);
+		p->numa_preferred_nid = xen_cpu_to_node(task_cpu(p));
 		return -EINVAL;
 	}

 	env.dst_nid = p->numa_preferred_nid;
-	dist = env.dist = node_distance(env.src_nid, env.dst_nid);
+	dist = env.dist = xen_numa_distance[env.src_nid*XEN_NUMNODES+env.dst_nid];
 	taskweight = task_weight(p, env.src_nid, dist);
 	groupweight = group_weight(p, env.src_nid, dist);
 	update_numa_stats(&env.src_stats, env.src_nid);
@@ -1804,16 +1810,19 @@ static int task_numa_migrate(struct task_struct *p)
 	 *   we need to check other locations.
 	 */
 	if (env.best_cpu == -1 || (p->numa_group && p->numa_group->active_nodes > 1)) {
-		for_each_online_node(nid) {
+		for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+			if (xen_memnode_map[nid]) {
 			if (nid == env.src_nid || nid == p->numa_preferred_nid)
 				continue;

-			dist = node_distance(env.src_nid, env.dst_nid);
+			dist = xen_numa_distance[env.src_nid*XEN_NUMNODES+env.dst_nid];
+			/*
 			if (sched_numa_topology_type == NUMA_BACKPLANE &&
 						dist != env.dist) {
 				taskweight = task_weight(p, env.src_nid, dist);
 				groupweight = group_weight(p, env.src_nid, dist);
 			}
+			*/

 			/* Only consider nodes where both task and groups benefit */
 			taskimp = task_weight(p, nid, dist) - taskweight;
@@ -1826,6 +1835,7 @@ static int task_numa_migrate(struct task_struct *p)
 			update_numa_stats(&env.dst_stats, env.dst_nid);
 			if (numa_has_capacity(&env))
 				task_numa_find_cpu(&env, taskimp, groupimp);
+			}
 		}
 	}

@@ -1887,7 +1897,7 @@ static void numa_migrate_preferred(struct task_struct *p)
 	p->numa_migrate_retry = jiffies + interval;

 	/* Success if task is already running on preferred CPU */
-	if (task_node(p) == p->numa_preferred_nid)
+	if (xen_cpu_to_node(task_cpu(p)) == p->numa_preferred_nid)
 		return;

 	/* Otherwise, try migrate to a CPU on the preferred node */
@@ -1905,16 +1915,20 @@ static void numa_group_count_active_nodes(struct numa_group *numa_group)
 	unsigned long faults, max_faults = 0;
 	int nid, active_nodes = 0;

-	for_each_online_node(nid) {
+	for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+		if (xen_memnode_map[nid]) {
 		faults = group_faults_cpu(numa_group, nid);
 		if (faults > max_faults)
 			max_faults = faults;
+		}
 	}

-	for_each_online_node(nid) {
+	for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+		if (xen_memnode_map[nid]) {
 		faults = group_faults_cpu(numa_group, nid);
 		if (faults * ACTIVE_NODE_FRACTION > max_faults)
 			active_nodes++;
+		}
 	}

 	numa_group->max_faults_cpu = max_faults;
@@ -2155,7 +2169,8 @@ static void task_numa_placement(struct task_struct *p)
 	}

 	/* Find the node with the highest number of faults */
-	for_each_online_node(nid) {
+	for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+		if (xen_memnode_map[nid]) {
 		/* Keep track of the offsets in numa_faults array */
 		int mem_idx, membuf_idx, cpu_idx, cpubuf_idx;
 		unsigned long faults = 0, group_faults = 0;
@@ -2215,6 +2230,7 @@ static void task_numa_placement(struct task_struct *p)
 			max_group_faults = group_faults;
 			max_group_nid = nid;
 		}
+		}
 	}

 	update_task_scan_period(p, fault_types[0], fault_types[1]);
@@ -2222,7 +2238,9 @@ static void task_numa_placement(struct task_struct *p)
 	if (p->numa_group) {
 		numa_group_count_active_nodes(p->numa_group);
 		spin_unlock_irq(group_lock);
-		max_nid = preferred_group_nid(p, max_group_nid);
+		/* We assume that there are direct connections between all NUMA nodes. */
+		/* max_nid = preferred_group_nid(p, max_group_nid); */
+		max_nid = max_group_nid;
 	}

 	if (max_faults) {
@@ -2230,7 +2248,7 @@ static void task_numa_placement(struct task_struct *p)
 		if (max_nid != p->numa_preferred_nid)
 			sched_setnuma(p, max_nid);

-		if (task_node(p) != p->numa_preferred_nid)
+		if (xen_cpu_to_node(task_cpu(p)) != p->numa_preferred_nid)
 			numa_migrate_preferred(p);
 	}
 }
@@ -2257,7 +2275,7 @@ static void task_numa_group(struct task_struct *p, int cpupid, int flags,

 	if (unlikely(!p->numa_group)) {
 		unsigned int size = sizeof(struct numa_group) +
-				    4*nr_node_ids*sizeof(unsigned long);
+				    4*xen_numa_num_nodes*sizeof(unsigned long);

 		grp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN);
 		if (!grp)
@@ -2270,9 +2288,9 @@ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
 		grp->gid = p->pid;
 		/* Second half of the array tracks nids where faults happen */
 		grp->faults_cpu = grp->faults + NR_NUMA_HINT_FAULT_TYPES *
-						nr_node_ids;
+						xen_numa_num_nodes;

-		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
+		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * xen_numa_num_nodes; i++)
 			grp->faults[i] = p->numa_faults[i];

 		grp->total_faults = p->total_numa_faults;
@@ -2330,7 +2348,7 @@ static void task_numa_group(struct task_struct *p, int cpupid, int flags,
 	BUG_ON(irqs_disabled());
 	double_lock_irq(&my_grp->lock, &grp->lock);

-	for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) {
+	for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * xen_numa_num_nodes; i++) {
 		my_grp->faults[i] -= p->numa_faults[i];
 		grp->faults[i] += p->numa_faults[i];
 	}
@@ -2362,7 +2380,7 @@ void task_numa_free(struct task_struct *p)

 	if (grp) {
 		spin_lock_irqsave(&grp->lock, flags);
-		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++)
+		for (i = 0; i < NR_NUMA_HINT_FAULT_STATS * xen_numa_num_nodes; i++)
 			grp->faults[i] -= p->numa_faults[i];
 		grp->total_faults -= p->total_numa_faults;

@@ -2383,7 +2401,7 @@ void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)
 {
 	struct task_struct *p = current;
 	bool migrated = flags & TNF_MIGRATED;
-	int cpu_node = task_node(current);
+	int cpu_node = xen_cpu_to_node(task_cpu(current));
 	int local = !!(flags & TNF_FAULT_LOCAL);
 	struct numa_group *ng;
 	int priv;
@@ -2398,7 +2416,7 @@ void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)
 	/* Allocate buffer to track faults on a per-node basis */
 	if (unlikely(!p->numa_faults)) {
 		int size = sizeof(*p->numa_faults) *
-			   NR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids;
+			   NR_NUMA_HINT_FAULT_BUCKETS * xen_numa_num_nodes;

 		p->numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN);
 		if (!p->numa_faults)
@@ -6644,8 +6662,8 @@ static int migrate_degrades_locality(struct task_struct *p, struct lb_env *env)
 	if (!p->numa_faults || !(env->sd->flags & SD_NUMA))
 		return -1;

-	src_nid = cpu_to_node(env->src_cpu);
-	dst_nid = cpu_to_node(env->dst_cpu);
+	src_nid = xen_cpu_to_node(env->src_cpu);
+	dst_nid = xen_cpu_to_node(env->dst_cpu);

 	if (src_nid == dst_nid)
 		return -1;
@@ -9574,7 +9592,8 @@ void show_numa_stats(struct task_struct *p, struct seq_file *m)
 	int node;
 	unsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0;

-	for_each_online_node(node) {
+	for (node = 0; node < xen_numa_num_nodes; node++) {
+		if (xen_memnode_map[node]) {
 		if (p->numa_faults) {
 			tsf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 0)];
 			tpf = p->numa_faults[task_faults_idx(NUMA_MEM, node, 1)];
@@ -9584,6 +9603,7 @@ void show_numa_stats(struct task_struct *p, struct seq_file *m)
 			gpf = p->numa_group->faults[task_faults_idx(NUMA_MEM, node, 1)];
 		}
 		print_numa_stats(m, node, tsf, tpf, gsf, gpf);
+		}
 	}
 }
 #endif /* CONFIG_NUMA_BALANCING */
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 1981ed6..18136d8 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1459,7 +1459,7 @@ int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t pmd)
 	struct page *page;
 	unsigned long haddr = vmf->address & HPAGE_PMD_MASK;
 	int page_nid = -1, this_nid = numa_node_id();
-	int target_nid, last_cpupid = -1;
+	int target_nid, xpv_target_nid, last_cpupid = -1;
 	bool page_locked;
 	bool migrated = false;
 	bool was_writable;
@@ -1503,7 +1503,7 @@ int do_huge_pmd_numa_page(struct vm_fault *vmf, pmd_t pmd)
 	 * page_table_lock if at all possible
 	 */
 	page_locked = trylock_page(page);
-	target_nid = mpol_misplaced(page, vma, haddr);
+	target_nid = mpol_misplaced(page, vma, haddr, &xpv_target_nid);
 	if (target_nid == -1) {
 		/* If the page was locked, there are no parallel migrations */
 		if (page_locked)
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 2d2ff5e..3b35524 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -1533,7 +1533,8 @@ static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,
 	gfp_mask |= __GFP_COMP|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;
 	if (nid == NUMA_NO_NODE)
 		nid = numa_mem_id();
-	return __alloc_pages_nodemask(gfp_mask, order, nid, nmask);
+	return __alloc_pages_nodemask(gfp_mask, order, nid,
+					nmask, NUMA_NO_NODE);
 }

 static struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,
diff --git a/mm/internal.h b/mm/internal.h
index 1df011f..e7d8c37 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -121,6 +121,7 @@ struct alloc_context {
 	int migratetype;
 	enum zone_type high_zoneidx;
 	bool spread_dirty_pages;
+	int pv_preferred_nid;
 };

 #define ac_classzone_idx(ac) zonelist_zone_idx(ac->preferred_zoneref)
diff --git a/mm/memory.c b/mm/memory.c
index a728bed..f43c3cf 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3693,7 +3693,7 @@ static int do_fault(struct vm_fault *vmf)

 static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 				unsigned long addr, int page_nid,
-				int *flags)
+				int *flags, int *xpv_target_nid)
 {
 	get_page(page);

@@ -3703,7 +3703,7 @@ static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 		*flags |= TNF_FAULT_LOCAL;
 	}

-	return mpol_misplaced(page, vma, addr);
+	return mpol_misplaced(page, vma, addr, xpv_target_nid);
 }

 static int do_numa_page(struct vm_fault *vmf)
@@ -3713,6 +3713,7 @@ static int do_numa_page(struct vm_fault *vmf)
 	int page_nid = -1;
 	int last_cpupid;
 	int target_nid;
+	int xpv_target_nid;
 	bool migrated = false;
 	pte_t pte;
 	bool was_writable = pte_savedwrite(vmf->orig_pte);
@@ -3773,19 +3774,20 @@ static int do_numa_page(struct vm_fault *vmf)
 		flags |= TNF_SHARED;

 	last_cpupid = page_cpupid_last(page);
-	page_nid = page_to_nid(page);
+	page_nid = page->machine_nid;
 	target_nid = numa_migrate_prep(page, vma, vmf->address, page_nid,
-			&flags);
+			&flags, &xpv_target_nid);
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
-	if (target_nid == -1) {
+	if (xpv_target_nid == -1) {
 		put_page(page);
 		goto out;
 	}

 	/* Migrate to the requested node */
-	migrated = migrate_misplaced_page(page, vma, target_nid);
+	migrated = migrate_misplaced_page(page, vma,
+				target_nid, xpv_target_nid);
 	if (migrated) {
-		page_nid = target_nid;
+		page_nid = xpv_target_nid;
 		flags |= TNF_MIGRATED;
 	} else
 		flags |= TNF_MIGRATE_FAIL;
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index a2af6d5..31c163b 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -102,6 +102,10 @@
 #include <asm/tlbflush.h>
 #include <linux/uaccess.h>

+#include <asm/xen/page.h>
+#include <asm/xen/vnuma.h>
+#include <xen/xen.h>
+
 #include "internal.h"

 /* Internal flags */
@@ -122,6 +126,7 @@
 	.refcnt = ATOMIC_INIT(1), /* never free it */
 	.mode = MPOL_PREFERRED,
 	.flags = MPOL_F_LOCAL,
+	.pv_preferred_node = NUMA_NO_NODE,
 };

 static struct mempolicy preferred_node_policy[MAX_NUMNODES];
@@ -203,6 +208,7 @@ static int mpol_set_nodemask(struct mempolicy *pol,
 		     const nodemask_t *nodes, struct nodemask_scratch *nsc)
 {
 	int ret;
+	nodemask_t uma;

 	/* if mode is MPOL_DEFAULT, pol is NULL. This is right. */
 	if (pol == NULL)
@@ -212,16 +218,20 @@ static int mpol_set_nodemask(struct mempolicy *pol,
 		  cpuset_current_mems_allowed, node_states[N_MEMORY]);

 	VM_BUG_ON(!nodes);
-	if (pol->mode == MPOL_PREFERRED && nodes_empty(*nodes))
+	if (pol->mode == MPOL_PREFERRED &&
+	     (nodes_empty(*nodes) || xen_pv_domu()))
 		nodes = NULL;	/* explicit local allocation */
 	else {
+		nodes_clear(uma);
+		node_set(0, uma);
+
 		if (pol->flags & MPOL_F_RELATIVE_NODES)
-			mpol_relative_nodemask(&nsc->mask2, nodes, &nsc->mask1);
+			mpol_relative_nodemask(&nsc->mask2, &uma, &nsc->mask1);
 		else
-			nodes_and(nsc->mask2, *nodes, nsc->mask1);
+			nodes_and(nsc->mask2, uma, nsc->mask1);

 		if (mpol_store_user_nodemask(pol))
-			pol->w.user_nodemask = *nodes;
+			pol->w.user_nodemask = uma;
 		else
 			pol->w.cpuset_mems_allowed =
 						cpuset_current_mems_allowed;
@@ -279,6 +289,19 @@ static struct mempolicy *mpol_new(unsigned short mode, unsigned short flags,
 	policy->mode = mode;
 	policy->flags = flags;

+	policy->pv_nodes = *nodes;
+
+	if (nodes && !nodes_empty(*nodes)) {
+		if (mode == MPOL_PREFERRED || mode == MPOL_BIND)
+			policy->pv_preferred_node = first_node(*nodes);
+		else
+			policy->pv_preferred_node = NUMA_NO_NODE;
+	} else
+		policy->pv_preferred_node = NUMA_NO_NODE;
+
+	pr_debug("setting pv_preferred_node %d\n",
+				policy->pv_preferred_node);
+
 	return policy;
 }

@@ -784,8 +807,10 @@ static long do_set_mempolicy(unsigned short mode, unsigned short flags,
 	}
 	old = current->mempolicy;
 	current->mempolicy = new;
-	if (new && new->mode == MPOL_INTERLEAVE)
-		current->il_prev = MAX_NUMNODES-1;
+	if (new && new->mode == MPOL_INTERLEAVE) {
+		current->pv_il_prev = MAX_NUMNODES-1;
+		current->il_prev = 0;
+	}
 	task_unlock(current);
 	mpol_put(old);
 	ret = 0;
@@ -809,11 +834,11 @@ static void get_policy_nodemask(struct mempolicy *p, nodemask_t *nodes)
 	case MPOL_BIND:
 		/* Fall through */
 	case MPOL_INTERLEAVE:
-		*nodes = p->v.nodes;
+		*nodes = p->pv_nodes;
 		break;
 	case MPOL_PREFERRED:
-		if (!(p->flags & MPOL_F_LOCAL))
-			node_set(p->v.preferred_node, *nodes);
+		if (p->pv_preferred_node != NUMA_NO_NODE)
+			node_set(p->pv_preferred_node, *nodes);
 		/* else return empty node mask for local allocation */
 		break;
 	default:
@@ -828,7 +853,7 @@ static int lookup_node(unsigned long addr)

 	err = get_user_pages(addr & PAGE_MASK, 1, 0, &p, NULL);
 	if (err >= 0) {
-		err = page_to_nid(p);
+		err = p->machine_nid;
 		put_page(p);
 	}
 	return err;
@@ -887,7 +912,7 @@ static long do_get_mempolicy(int *policy, nodemask_t *nmask,
 			*policy = err;
 		} else if (pol == current->mempolicy &&
 				pol->mode == MPOL_INTERLEAVE) {
-			*policy = next_node_in(current->il_prev, pol->v.nodes);
+			*policy = next_node_in(current->pv_il_prev, pol->pv_nodes);
 		} else {
 			err = -EINVAL;
 			goto out;
@@ -1694,6 +1719,17 @@ static unsigned interleave_nodes(struct mempolicy *policy)
 	return next;
 }

+static unsigned pv_interleave_nodes(struct mempolicy *policy)
+{
+	unsigned next;
+	struct task_struct *me = current;
+
+	next = next_node_in(me->pv_il_prev, policy->pv_nodes);
+	if (next < XEN_NUMNODES)
+		me->pv_il_prev = next;
+	return next;
+}
+
 /*
  * Depending on the memory policy provide a node from which to allocate the
  * next slab entry.
@@ -1761,6 +1797,22 @@ static unsigned offset_il_node(struct mempolicy *pol, unsigned long n)
 	return nid;
 }

+static unsigned pv_offset_il_node(struct mempolicy *pol, unsigned long n)
+{
+	unsigned nnodes = nodes_weight(pol->pv_nodes);
+	unsigned target;
+	int i;
+	int nid;
+
+	if (!nnodes)
+		return NUMA_NO_NODE;
+	target = (unsigned int)n % nnodes;
+	nid = first_node(pol->pv_nodes);
+	for (i = 0; i < target; i++)
+		nid = next_node(nid, pol->pv_nodes);
+	return nid;
+}
+
 /* Determine a node number for interleave */
 static inline unsigned interleave_nid(struct mempolicy *pol,
 		 struct vm_area_struct *vma, unsigned long addr, int shift)
@@ -1783,6 +1835,20 @@ static inline unsigned interleave_nid(struct mempolicy *pol,
 		return interleave_nodes(pol);
 }

+static inline unsigned pv_interleave_nid(struct mempolicy *pol,
+		 struct vm_area_struct *vma, unsigned long addr, int shift)
+{
+	if (vma) {
+		unsigned long off;
+
+		BUG_ON(shift < PAGE_SHIFT);
+		off = vma->vm_pgoff >> (shift - PAGE_SHIFT);
+		off += (addr - vma->vm_start) >> shift;
+		return pv_offset_il_node(pol, off);
+	} else
+		return pv_interleave_nodes(pol);
+}
+
 #ifdef CONFIG_HUGETLBFS
 /*
  * huge_node(@vma, @addr, @gfp_flags, @mpol)
@@ -1915,11 +1981,11 @@ bool mempolicy_nodemask_intersects(struct task_struct *tsk,
 /* Allocate a page in interleaved policy.
    Own path because it needs to do special accounting. */
 static struct page *alloc_page_interleave(gfp_t gfp, unsigned order,
-					unsigned nid)
+					unsigned nid, int pv_nid)
 {
 	struct page *page;

-	page = __alloc_pages(gfp, order, nid);
+	page = __alloc_pages_nodemask(gfp, order, nid, NULL, pv_nid);
 	if (page && page_to_nid(page) == nid) {
 		preempt_disable();
 		__inc_numa_state(page_zone(page), NUMA_INTERLEAVE_HIT);
@@ -1965,14 +2031,15 @@ struct page *
 	if (pol->mode == MPOL_INTERLEAVE) {
 		unsigned nid;

-		nid = interleave_nid(pol, vma, addr, PAGE_SHIFT + order);
+		nid = pv_interleave_nid(pol, vma, addr, PAGE_SHIFT + order);
 		mpol_cond_put(pol);
-		page = alloc_page_interleave(gfp, order, nid);
+		page = alloc_page_interleave(gfp, order, 0 /* nid */, nid);
 		goto out;
 	}

 	if (unlikely(IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE) && hugepage)) {
 		int hpage_node = node;
+		int pv_hpage_node = NUMA_NO_NODE;

 		/*
 		 * For hugepage allocation and non-interleave policy which
@@ -1984,22 +2051,25 @@ struct page *
 		 * If the policy is interleave, or does not allow the current
 		 * node in its nodemask, we allocate the standard way.
 		 */
-		if (pol->mode == MPOL_PREFERRED &&
-						!(pol->flags & MPOL_F_LOCAL))
-			hpage_node = pol->v.preferred_node;
+		if (pol->mode == MPOL_PREFERRED) {
+			pv_hpage_node = pol->pv_preferred_node;
+			if (!(pol->flags & MPOL_F_LOCAL))
+				hpage_node = pol->v.preferred_node;
+		}

 		nmask = policy_nodemask(gfp, pol);
 		if (!nmask || node_isset(hpage_node, *nmask)) {
 			mpol_cond_put(pol);
-			page = __alloc_pages_node(hpage_node,
-						gfp | __GFP_THISNODE, order);
+			page = __alloc_pages_nodemask(gfp | __GFP_THISNODE, order,
+					hpage_node, NULL, pv_hpage_node);
 			goto out;
 		}
 	}

 	nmask = policy_nodemask(gfp, pol);
 	preferred_nid = policy_node(gfp, pol, node);
-	page = __alloc_pages_nodemask(gfp, order, preferred_nid, nmask);
+	page = __alloc_pages_nodemask(gfp, order, preferred_nid,
+			nmask, pol->pv_preferred_node);
 	mpol_cond_put(pol);
 out:
 	return page;
@@ -2033,11 +2103,14 @@ struct page *alloc_pages_current(gfp_t gfp, unsigned order)
 	 * nor system default_policy
 	 */
 	if (pol->mode == MPOL_INTERLEAVE)
-		page = alloc_page_interleave(gfp, order, interleave_nodes(pol));
+		page = alloc_page_interleave(gfp, order,
+				0 /* interleave_nodes(pol) */,
+				pv_interleave_nodes(pol));
 	else
 		page = __alloc_pages_nodemask(gfp, order,
 				policy_node(gfp, pol, numa_node_id()),
-				policy_nodemask(gfp, pol));
+				policy_nodemask(gfp, pol),
+				pol->pv_preferred_node);

 	return page;
 }
@@ -2105,9 +2178,11 @@ bool __mpol_equal(struct mempolicy *a, struct mempolicy *b)
 	case MPOL_BIND:
 		/* Fall through */
 	case MPOL_INTERLEAVE:
-		return !!nodes_equal(a->v.nodes, b->v.nodes);
+		return !! (nodes_equal(a->v.nodes, b->v.nodes) &&
+			nodes_equal(a->pv_nodes, b->pv_nodes));
 	case MPOL_PREFERRED:
-		return a->v.preferred_node == b->v.preferred_node;
+		return (a->v.preferred_node == b->v.preferred_node &&
+			a->pv_preferred_node == b->pv_preferred_node);
 	default:
 		BUG();
 		return false;
@@ -2225,17 +2300,19 @@ static void sp_free(struct sp_node *n)
  * Policy determination "mimics" alloc_page_vma().
  * Called from fault path where we know the vma and faulting address.
  */
-int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)
+int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr,
+			int* xpv_polnid)
 {
 	struct mempolicy *pol;
 	struct zoneref *z;
-	int curnid = page_to_nid(page);
+	int xpv_curnid = page->machine_nid;
 	unsigned long pgoff;
 	int thiscpu = raw_smp_processor_id();
-	int thisnid = cpu_to_node(thiscpu);
-	int polnid = -1;
+	int xpv_thisnid = xen_cpu_to_node(thiscpu);
 	int ret = -1;

+	*xpv_polnid = -1;
+
 	pol = get_vma_policy(vma, addr);
 	if (!(pol->flags & MPOL_F_MOF))
 		goto out;
@@ -2244,31 +2321,20 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long
 	case MPOL_INTERLEAVE:
 		pgoff = vma->vm_pgoff;
 		pgoff += (addr - vma->vm_start) >> PAGE_SHIFT;
-		polnid = offset_il_node(pol, pgoff);
+		*xpv_polnid = pv_offset_il_node(pol, pgoff);
 		break;

 	case MPOL_PREFERRED:
 		if (pol->flags & MPOL_F_LOCAL)
-			polnid = numa_node_id();
+			*xpv_polnid = xpv_thisnid;
 		else
-			polnid = pol->v.preferred_node;
+			*xpv_polnid = pol->pv_preferred_node;
 		break;

 	case MPOL_BIND:
-
-		/*
-		 * allows binding to multiple nodes.
-		 * use current page if in policy nodemask,
-		 * else select nearest allowed node, if any.
-		 * If no allowed nodes, use current [!misplaced].
-		 */
-		if (node_isset(curnid, pol->v.nodes))
+		if (node_isset(xpv_curnid, pol->pv_nodes))
 			goto out;
-		z = first_zones_zonelist(
-				node_zonelist(numa_node_id(), GFP_HIGHUSER),
-				gfp_zone(GFP_HIGHUSER),
-				&pol->v.nodes);
-		polnid = z->zone->node;
+		*xpv_polnid = pol->pv_preferred_node;
 		break;

 	default:
@@ -2277,14 +2343,16 @@ int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long

 	/* Migrate the page towards the node whose CPU is referencing it */
 	if (pol->flags & MPOL_F_MORON) {
-		polnid = thisnid;
+		*xpv_polnid = xpv_thisnid;

-		if (!should_numa_migrate_memory(current, page, curnid, thiscpu))
+		if (!should_numa_migrate_memory(current, page, xpv_curnid, thiscpu)) {
+			*xpv_polnid = -1;
 			goto out;
+		}
 	}

-	if (curnid != polnid)
-		ret = polnid;
+	if (xpv_curnid == *xpv_polnid)
+		*xpv_polnid = -1;
 out:
 	mpol_cond_put(pol);

@@ -2568,6 +2636,7 @@ void __init numa_policy_init(void)
 			.refcnt = ATOMIC_INIT(1),
 			.mode = MPOL_PREFERRED,
 			.flags = MPOL_F_MOF | MPOL_F_MORON,
+			.pv_preferred_node = NUMA_NO_NODE,
 			.v = { .preferred_node = nid, },
 		};
 	}
diff --git a/mm/migrate.c b/mm/migrate.c
index 1236449..a4a2965 100644
--- a/mm/migrate.c
+++ b/mm/migrate.c
@@ -1835,6 +1835,7 @@ static bool migrate_balanced_pgdat(struct pglist_data *pgdat,
 	return false;
 }

+/*
 static struct page *alloc_misplaced_dst_page(struct page *page,
 					   unsigned long data,
 					   int **result)
@@ -1850,6 +1851,22 @@ static struct page *alloc_misplaced_dst_page(struct page *page,

 	return newpage;
 }
+*/
+
+static struct page *xpv_alloc_misplaced_dst_page(struct page *page,
+                                           unsigned long data,
+                                           int **result)
+{
+	int nid = (int) data;
+	struct page *newpage;
+
+	newpage = __alloc_pages_nodemask((GFP_HIGHUSER_MOVABLE |
+					 __GFP_THISNODE | __GFP_NOMEMALLOC |
+					 __GFP_NORETRY  | __GFP_NOWARN) &
+					 ~__GFP_RECLAIM, 0, 0, NULL, nid);
+
+	return newpage;
+}

 /*
  * page migration rate limiting control.
@@ -1898,8 +1915,10 @@ static int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)
 	VM_BUG_ON_PAGE(compound_order(page) && !PageTransHuge(page), page);

 	/* Avoid migrating to a node that is nearly full */
+	/*
 	if (!migrate_balanced_pgdat(pgdat, 1UL << compound_order(page)))
 		return 0;
+	*/

 	if (isolate_lru_page(page))
 		return 0;
@@ -1941,7 +1960,7 @@ bool pmd_trans_migrating(pmd_t pmd)
  * the page that will be dropped by this function before returning.
  */
 int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
-			   int node)
+			   int node, int xpv_node)
 {
 	pg_data_t *pgdat = NODE_DATA(node);
 	int isolated;
@@ -1960,17 +1979,21 @@ int migrate_misplaced_page(struct page *page, struct vm_area_struct *vma,
 	 * Rate-limit the amount of data that is being migrated to a node.
 	 * Optimal placement is no good if the memory bus is saturated and
 	 * all the time is being spent migrating!
+	 *
+	 * XPV: We don't have a separate pgdat!
 	 */
+	/*
 	if (numamigrate_update_ratelimit(pgdat, 1))
 		goto out;
+	*/

 	isolated = numamigrate_isolate_page(pgdat, page);
 	if (!isolated)
 		goto out;

 	list_add(&page->lru, &migratepages);
-	nr_remaining = migrate_pages(&migratepages, alloc_misplaced_dst_page,
-				     NULL, node, MIGRATE_ASYNC,
+	nr_remaining = migrate_pages(&migratepages, xpv_alloc_misplaced_dst_page,
+				     NULL, xpv_node, MIGRATE_ASYNC,
 				     MR_NUMA_MISPLACED);
 	if (nr_remaining) {
 		if (!list_empty(&migratepages)) {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 77e4d3c..449f7e8 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -69,6 +69,10 @@
 #include <linux/lockdep.h>
 #include <linux/nmi.h>

+#include <asm/xen/page.h>
+#include <asm/xen/vnuma.h>
+#include <xen/xen.h>
+
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
 #include <asm/div64.h>
@@ -805,6 +809,7 @@ static inline void __free_one_page(struct page *page,
 	unsigned long uninitialized_var(buddy_pfn);
 	struct page *buddy;
 	unsigned int max_order;
+	int page_nid;

 	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);

@@ -835,7 +840,7 @@ static inline void __free_one_page(struct page *page,
 			clear_page_guard(zone, buddy, order, migratetype);
 		} else {
 			list_del(&buddy->lru);
-			zone->free_area[order].nr_free--;
+			zone->numa_free_area[buddy->machine_nid][order].nr_free--;
 			rmv_page_order(buddy);
 		}
 		combined_pfn = buddy_pfn & pfn;
@@ -870,6 +875,7 @@ static inline void __free_one_page(struct page *page,

 done_merging:
 	set_page_order(page, order);
+	page_nid = page->machine_nid;

 	/*
 	 * If this is not the largest possible page, check if the buddy
@@ -888,14 +894,14 @@ static inline void __free_one_page(struct page *page,
 		if (pfn_valid_within(buddy_pfn) &&
 		    page_is_buddy(higher_page, higher_buddy, order + 1)) {
 			list_add_tail(&page->lru,
-				&zone->free_area[order].free_list[migratetype]);
+				&zone->numa_free_area[page_nid][order].free_list[migratetype]);
 			goto out;
 		}
 	}

-	list_add(&page->lru, &zone->free_area[order].free_list[migratetype]);
+	list_add(&page->lru, &zone->numa_free_area[page_nid][order].free_list[migratetype]);
 out:
-	zone->free_area[order].nr_free++;
+	zone->numa_free_area[page_nid][order].nr_free++;
 }

 /*
@@ -1181,6 +1187,7 @@ static void __meminit __init_single_page(struct page *page, unsigned long pfn,
 	if (!is_highmem_idx(zone))
 		set_page_address(page, __va(pfn << PAGE_SHIFT));
 #endif
+	page->machine_nid = xen_pfn_to_nid(pfn);
 }

 static void __meminit __init_single_pfn(unsigned long pfn, unsigned long zone,
@@ -1641,13 +1648,13 @@ void __init init_cma_reserved_pageblock(struct page *page)
  * -- nyc
  */
 static inline void expand(struct zone *zone, struct page *page,
-	int low, int high, struct free_area *area,
+	int low, int high, /* struct free_area *area, */
 	int migratetype)
 {
 	unsigned long size = 1 << high;
+	struct free_area *area;

 	while (high > low) {
-		area--;
 		high--;
 		size >>= 1;
 		VM_BUG_ON_PAGE(bad_range(zone, &page[size]), &page[size]);
@@ -1661,6 +1668,7 @@ static inline void expand(struct zone *zone, struct page *page,
 		if (set_page_guard(zone, &page[size], high, migratetype))
 			continue;

+		area = &zone->numa_free_area[page[size].machine_nid][high];
 		list_add(&page[size].lru, &area->free_list[migratetype]);
 		area->nr_free++;
 		set_page_order(&page[size], high);
@@ -1794,15 +1802,19 @@ static void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags
  */
 static inline
 struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
-						int migratetype)
+				int migratetype, int pv_preferred_nid)
 {
 	unsigned int current_order;
 	struct free_area *area;
 	struct page *page;
+	int nid, i = xen_numa_num_nodes - 1;

+	do {
+	nid = nodelists[pv_preferred_nid][i];
+	if (xen_memnode_map[nid]) {
 	/* Find a page of the appropriate size in the preferred list */
 	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
-		area = &(zone->free_area[current_order]);
+		area = &(zone->numa_free_area[nid][current_order]);
 		page = list_first_entry_or_null(&area->free_list[migratetype],
 							struct page, lru);
 		if (!page)
@@ -1810,10 +1822,12 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 		list_del(&page->lru);
 		rmv_page_order(page);
 		area->nr_free--;
-		expand(zone, page, order, current_order, area, migratetype);
+		expand(zone, page, order, current_order, /* area, */migratetype);
 		set_pcppage_migratetype(page, migratetype);
 		return page;
 	}
+	}
+	} while (i--);

 	return NULL;
 }
@@ -1837,13 +1851,15 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,

 #ifdef CONFIG_CMA
 static struct page *__rmqueue_cma_fallback(struct zone *zone,
-					unsigned int order)
+					unsigned int order,
+					int pv_preferred_nid)
 {
-	return __rmqueue_smallest(zone, order, MIGRATE_CMA);
+	return __rmqueue_smallest(zone, order, MIGRATE_CMA, pv_preferred_nid);
 }
 #else
 static inline struct page *__rmqueue_cma_fallback(struct zone *zone,
-					unsigned int order) { return NULL; }
+					unsigned int order,
+					int pv_preferred_nid) { return NULL; }
 #endif

 /*
@@ -1898,7 +1914,7 @@ static int move_freepages(struct zone *zone,

 		order = page_order(page);
 		list_move(&page->lru,
-			  &zone->free_area[order].free_list[migratetype]);
+			  &zone->numa_free_area[page->machine_nid][order].free_list[migratetype]);
 		page += 1 << order;
 		pages_moved += 1 << order;
 	}
@@ -2046,7 +2062,7 @@ static void steal_suitable_fallback(struct zone *zone, struct page *page,
 	return;

 single_page:
-	area = &zone->free_area[current_order];
+	area = &zone->numa_free_area[page->machine_nid][current_order];
 	list_move(&page->lru, &area->free_list[start_type]);
 }

@@ -2141,7 +2157,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 	struct zoneref *z;
 	struct zone *zone;
 	struct page *page;
-	int order;
+	int order, nid, i;
 	bool ret;

 	for_each_zone_zonelist_nodemask(zone, z, zonelist, ac->high_zoneidx,
@@ -2155,9 +2171,12 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 			continue;

 		spin_lock_irqsave(&zone->lock, flags);
+		i = xen_numa_num_nodes - 1;
+		do {
+		nid = nodelists[ac->pv_preferred_nid][i];
+		if (xen_memnode_map[nid]) {
 		for (order = 0; order < MAX_ORDER; order++) {
-			struct free_area *area = &(zone->free_area[order]);
-
+			struct free_area *area = &(zone->numa_free_area[nid][order]);
 			page = list_first_entry_or_null(
 					&area->free_list[MIGRATE_HIGHATOMIC],
 					struct page, lru);
@@ -2201,6 +2220,8 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 				return ret;
 			}
 		}
+		}
+		} while (i--);
 		spin_unlock_irqrestore(&zone->lock, flags);
 	}

@@ -2218,24 +2239,30 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
  * condition simpler.
  */
 static inline bool
-__rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
+__rmqueue_fallback(struct zone *zone, int order, int start_migratetype,
+			int pv_preferred_nid)
 {
 	struct free_area *area;
 	int current_order;
 	struct page *page;
 	int fallback_mt;
 	bool can_steal;
+	int nid, i = xen_numa_num_nodes - 1;

 	/*
 	 * Find the largest available free page in the other list. This roughly
 	 * approximates finding the pageblock with the most free pages, which
 	 * would be too costly to do exactly.
 	 */
+	do {
+	nid = nodelists[pv_preferred_nid][i];
+	if (xen_memnode_map[nid]) {
 	for (current_order = MAX_ORDER - 1; current_order >= order;
 				--current_order) {
-		area = &(zone->free_area[current_order]);
+		area = &(zone->numa_free_area[nid][current_order]);
 		fallback_mt = find_suitable_fallback(area, current_order,
 				start_migratetype, false, &can_steal);
+
 		if (fallback_mt == -1)
 			continue;

@@ -2253,18 +2280,26 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,

 		goto do_steal;
 	}
+	}
+	} while (i--);

 	return false;

 find_smallest:
+	i = xen_numa_num_nodes - 1;
+	do {
+	nid = nodelists[pv_preferred_nid][i];
+	if (xen_memnode_map[nid]) {
 	for (current_order = order; current_order < MAX_ORDER;
 							current_order++) {
-		area = &(zone->free_area[current_order]);
+		area = &(zone->numa_free_area[nid][current_order]);
 		fallback_mt = find_suitable_fallback(area, current_order,
-				start_migratetype, false, &can_steal);
+			start_migratetype, false, &can_steal);
 		if (fallback_mt != -1)
 			break;
 	}
+	}
+	} while (i--);

 	/*
 	 * This should not happen - we already found a suitable fallback
@@ -2290,17 +2325,17 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
  * Call me with the zone->lock already held.
  */
 static struct page *__rmqueue(struct zone *zone, unsigned int order,
-				int migratetype)
+				int migratetype, int pv_preferred_nid)
 {
 	struct page *page;

 retry:
-	page = __rmqueue_smallest(zone, order, migratetype);
+	page = __rmqueue_smallest(zone, order, migratetype, pv_preferred_nid);
 	if (unlikely(!page)) {
 		if (migratetype == MIGRATE_MOVABLE)
-			page = __rmqueue_cma_fallback(zone, order);
+			page = __rmqueue_cma_fallback(zone, order, pv_preferred_nid);

-		if (!page && __rmqueue_fallback(zone, order, migratetype))
+		if (!page && __rmqueue_fallback(zone, order, migratetype, pv_preferred_nid))
 			goto retry;
 	}

@@ -2315,13 +2350,13 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
  */
 static int rmqueue_bulk(struct zone *zone, unsigned int order,
 			unsigned long count, struct list_head *list,
-			int migratetype, bool cold)
+			int migratetype, /* bool cold, */ int pv_preferred_nid)
 {
 	int i, alloced = 0;

 	spin_lock(&zone->lock);
 	for (i = 0; i < count; ++i) {
-		struct page *page = __rmqueue(zone, order, migratetype);
+		struct page *page = __rmqueue(zone, order, migratetype, pv_preferred_nid);
 		if (unlikely(page == NULL))
 			break;

@@ -2330,18 +2365,21 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,

 		/*
 		 * Split buddy pages returned by expand() are received here
-		 * in physical page order. The page is added to the callers and
-		 * list and the list head then moves forward. From the callers
+		 * in physical page order. The page is added to the tail of
+		 * the caller's list. From the callers
 		 * perspective, the linked list is ordered by page number in
 		 * some conditions. This is useful for IO devices that can
 		 * merge IO requests if the physical pages are ordered
 		 * properly.
 		 */
+		/*
 		if (likely(!cold))
 			list_add(&page->lru, list);
 		else
 			list_add_tail(&page->lru, list);
 		list = &page->lru;
+		*/
+		list_add_tail(&page->lru, list);
 		alloced++;
 		if (is_migrate_cma(get_pcppage_migratetype(page)))
 			__mod_zone_page_state(zone, NR_FREE_CMA_PAGES,
@@ -2396,14 +2434,16 @@ static void drain_pages_zone(unsigned int cpu, struct zone *zone)
 	unsigned long flags;
 	struct per_cpu_pageset *pset;
 	struct per_cpu_pages *pcp;
+	int nid;

 	local_irq_save(flags);
 	pset = per_cpu_ptr(zone->pageset, cpu);
-
-	pcp = &pset->pcp;
-	if (pcp->count) {
-		free_pcppages_bulk(zone, pcp->count, pcp);
-		pcp->count = 0;
+	for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+		pcp = &pset->numa_pcp[nid];
+		if (pcp->count) {
+			free_pcppages_bulk(zone, pcp->count, pcp);
+			pcp->count = 0;
+		}
 	}
 	local_irq_restore(flags);
 }
@@ -2503,21 +2543,26 @@ void drain_all_pages(struct zone *zone)
 		struct per_cpu_pageset *pcp;
 		struct zone *z;
 		bool has_pcps = false;
+		int nid;

 		if (zone) {
 			pcp = per_cpu_ptr(zone->pageset, cpu);
-			if (pcp->pcp.count)
-				has_pcps = true;
-		} else {
-			for_each_populated_zone(z) {
-				pcp = per_cpu_ptr(z->pageset, cpu);
-				if (pcp->pcp.count) {
+			for (nid = 0; nid < xen_numa_num_nodes; nid++)
+				if (pcp->numa_pcp[nid].count) {
 					has_pcps = true;
 					break;
 				}
+		} else {
+			for_each_populated_zone(z) {
+				pcp = per_cpu_ptr(z->pageset, cpu);
+				for (nid = 0; nid < xen_numa_num_nodes; nid++)
+					if (pcp->numa_pcp[nid].count) {
+						has_pcps = true;
+						goto zone_has_pcps;
+					}
 			}
 		}
-
+zone_has_pcps:
 		if (has_pcps)
 			cpumask_set_cpu(cpu, &cpus_with_pcps);
 		else
@@ -2548,6 +2593,7 @@ void mark_free_pages(struct zone *zone)
 	unsigned long flags;
 	unsigned int order, t;
 	struct page *page;
+	int nid = xen_numa_num_nodes - 1;

 	if (zone_is_empty(zone))
 		return;
@@ -2571,9 +2617,11 @@ void mark_free_pages(struct zone *zone)
 				swsusp_unset_page_free(page);
 		}

+	do {
+	if (xen_memnode_map[nid]) {
 	for_each_migratetype_order(order, t) {
 		list_for_each_entry(page,
-				&zone->free_area[order].free_list[t], lru) {
+				&zone->numa_free_area[nid][order].free_list[t], lru) {
 			unsigned long i;

 			pfn = page_to_pfn(page);
@@ -2586,6 +2634,8 @@ void mark_free_pages(struct zone *zone)
 			}
 		}
 	}
+	}
+	} while (nid--);
 	spin_unlock_irqrestore(&zone->lock, flags);
 }
 #endif /* CONFIG_PM */
@@ -2625,11 +2675,14 @@ void free_hot_cold_page(struct page *page, bool cold)
 		migratetype = MIGRATE_MOVABLE;
 	}

-	pcp = &this_cpu_ptr(zone->pageset)->pcp;
+	pcp = &this_cpu_ptr(zone->pageset)->numa_pcp[page->machine_nid];
+	/*
 	if (!cold)
 		list_add(&page->lru, &pcp->lists[migratetype]);
 	else
 		list_add_tail(&page->lru, &pcp->lists[migratetype]);
+	*/
+	list_add(&page->lru, &pcp->lists[migratetype]);
 	pcp->count++;
 	if (pcp->count >= pcp->high) {
 		unsigned long batch = READ_ONCE(pcp->batch);
@@ -2711,7 +2764,7 @@ int __isolate_free_page(struct page *page, unsigned int order)

 	/* Remove page from free list */
 	list_del(&page->lru);
-	zone->free_area[order].nr_free--;
+	zone->numa_free_area[page->machine_nid][order].nr_free--;
 	rmv_page_order(page);

 	/*
@@ -2758,8 +2811,8 @@ static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)

 /* Remove page from the per-cpu list, caller must protect the list */
 static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
-			bool cold, struct per_cpu_pages *pcp,
-			struct list_head *list)
+			/* bool cold, */ struct per_cpu_pages *pcp,
+			struct list_head *list, int pv_preferred_nid)
 {
 	struct page *page;

@@ -2767,15 +2820,19 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 		if (list_empty(list)) {
 			pcp->count += rmqueue_bulk(zone, 0,
 					pcp->batch, list,
-					migratetype, cold);
+					migratetype, /* cold, */
+					pv_preferred_nid);
 			if (unlikely(list_empty(list)))
 				return NULL;
 		}

+		/*
 		if (cold)
 			page = list_last_entry(list, struct page, lru);
 		else
 			page = list_first_entry(list, struct page, lru);
+		*/
+		page = list_first_entry(list, struct page, lru);

 		list_del(&page->lru);
 		pcp->count--;
@@ -2787,18 +2844,19 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 /* Lock and remove page from the per-cpu list */
 static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 			struct zone *zone, unsigned int order,
-			gfp_t gfp_flags, int migratetype)
+			gfp_t gfp_flags, int migratetype,
+			int pv_preferred_nid)
 {
 	struct per_cpu_pages *pcp;
 	struct list_head *list;
-	bool cold = ((gfp_flags & __GFP_COLD) != 0);
+	/* bool cold = ((gfp_flags & __GFP_COLD) != 0); */
 	struct page *page;
 	unsigned long flags;

 	local_irq_save(flags);
-	pcp = &this_cpu_ptr(zone->pageset)->pcp;
+	pcp = &this_cpu_ptr(zone->pageset)->numa_pcp[pv_preferred_nid];
 	list = &pcp->lists[migratetype];
-	page = __rmqueue_pcplist(zone,  migratetype, cold, pcp, list);
+	page = __rmqueue_pcplist(zone,  migratetype, /* cold, */ pcp, list, pv_preferred_nid);
 	if (page) {
 		__count_zid_vm_events(PGALLOC, page_zonenum(page), 1 << order);
 		zone_statistics(preferred_zone, zone);
@@ -2814,14 +2872,14 @@ static struct page *rmqueue_pcplist(struct zone *preferred_zone,
 struct page *rmqueue(struct zone *preferred_zone,
 			struct zone *zone, unsigned int order,
 			gfp_t gfp_flags, unsigned int alloc_flags,
-			int migratetype)
+			int migratetype, int pv_preferred_nid)
 {
 	unsigned long flags;
 	struct page *page;

 	if (likely(order == 0)) {
 		page = rmqueue_pcplist(preferred_zone, zone, order,
-				gfp_flags, migratetype);
+				gfp_flags, migratetype, pv_preferred_nid);
 		goto out;
 	}

@@ -2835,12 +2893,12 @@ struct page *rmqueue(struct zone *preferred_zone,
 	do {
 		page = NULL;
 		if (alloc_flags & ALLOC_HARDER) {
-			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
+			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC, pv_preferred_nid);
 			if (page)
 				trace_mm_page_alloc_zone_locked(page, order, migratetype);
 		}
 		if (!page)
-			page = __rmqueue(zone, order, migratetype);
+			page = __rmqueue(zone, order, migratetype, pv_preferred_nid);
 	} while (page && check_new_pages(page, order));
 	spin_unlock(&zone->lock);
 	if (!page)
@@ -2952,6 +3010,7 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 	long min = mark;
 	int o;
 	const bool alloc_harder = (alloc_flags & (ALLOC_HARDER|ALLOC_OOM));
+	int nid = xen_numa_num_nodes - 1;

 	/* free_pages may go negative - that's OK */
 	free_pages -= (1 << order) - 1;
@@ -2998,9 +3057,11 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 	if (!order)
 		return true;

+	do {
+	if (xen_memnode_map[nid]) {
 	/* For a high-order request, check at least one suitable page is free */
 	for (o = order; o < MAX_ORDER; o++) {
-		struct free_area *area = &z->free_area[o];
+		struct free_area *area = &z->numa_free_area[nid][o];
 		int mt;

 		if (!area->nr_free)
@@ -3021,6 +3082,8 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 		}
 #endif
 	}
+	}
+	} while (nid--);
 	return false;
 }

@@ -3170,7 +3233,8 @@ static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)

 try_this_zone:
 		page = rmqueue(ac->preferred_zoneref->zone, zone, order,
-				gfp_mask, alloc_flags, ac->migratetype);
+				gfp_mask, alloc_flags, ac->migratetype,
+				ac->pv_preferred_nid);
 		if (page) {
 			prep_new_page(page, order, gfp_mask, alloc_flags);

@@ -4129,13 +4193,17 @@ bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)
 static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,
 		int preferred_nid, nodemask_t *nodemask,
 		struct alloc_context *ac, gfp_t *alloc_mask,
-		unsigned int *alloc_flags)
+		unsigned int *alloc_flags, int pv_preferred_nid)
 {
 	ac->high_zoneidx = gfp_zone(gfp_mask);
 	ac->zonelist = node_zonelist(preferred_nid, gfp_mask);
 	ac->nodemask = nodemask;
 	ac->migratetype = gfpflags_to_migratetype(gfp_mask);

+	ac->pv_preferred_nid = pv_preferred_nid;
+	if (ac->pv_preferred_nid == NUMA_NO_NODE)
+		ac->pv_preferred_nid = xen_cpu_to_node(smp_processor_id());
+
 	if (cpusets_enabled()) {
 		*alloc_mask |= __GFP_HARDWALL;
 		if (!ac->nodemask)
@@ -4179,7 +4247,7 @@ static inline void finalise_ac(gfp_t gfp_mask,
  */
 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
-							nodemask_t *nodemask)
+				nodemask_t *nodemask, int pv_preferred_nid)
 {
 	struct page *page;
 	unsigned int alloc_flags = ALLOC_WMARK_LOW;
@@ -4188,7 +4256,8 @@ struct page *

 	gfp_mask &= gfp_allowed_mask;
 	alloc_mask = gfp_mask;
-	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &ac, &alloc_mask, &alloc_flags))
+	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask,
+			&ac, &alloc_mask, &alloc_flags, pv_preferred_nid))
 		return NULL;

 	finalise_ac(gfp_mask, order, &ac);
@@ -5377,10 +5446,19 @@ void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 static void __meminit zone_init_free_lists(struct zone *zone)
 {
 	unsigned int order, t;
+	int nid = xen_numa_num_nodes - 1;
+
 	for_each_migratetype_order(order, t) {
 		INIT_LIST_HEAD(&zone->free_area[order].free_list[t]);
 		zone->free_area[order].nr_free = 0;
 	}
+
+	do {
+		for_each_migratetype_order(order, t) {
+			INIT_LIST_HEAD(&zone->numa_free_area[nid][order].free_list[t]);
+			zone->numa_free_area[nid][order].nr_free = 0;
+		}
+	} while (nid--);
 }

 #ifndef __HAVE_ARCH_MEMMAP_INIT
@@ -5468,13 +5546,21 @@ static void pageset_update(struct per_cpu_pages *pcp, unsigned long high,
 /* a companion to pageset_set_high() */
 static void pageset_set_batch(struct per_cpu_pageset *p, unsigned long batch)
 {
-	pageset_update(&p->pcp, 6 * batch, max(1UL, 1 * batch));
+	int nid;
+	unsigned long high = 6 * batch;
+	unsigned long safe_batch = max(1UL, 1 * batch);
+
+	pageset_update(&p->pcp, high, safe_batch);
+
+	for (nid = 0; nid < xen_numa_num_nodes; nid++)
+		pageset_update(&p->numa_pcp[nid], high, safe_batch);
 }

 static void pageset_init(struct per_cpu_pageset *p)
 {
 	struct per_cpu_pages *pcp;
 	int migratetype;
+	int nid;

 	memset(p, 0, sizeof(*p));

@@ -5482,6 +5568,13 @@ static void pageset_init(struct per_cpu_pageset *p)
 	pcp->count = 0;
 	for (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)
 		INIT_LIST_HEAD(&pcp->lists[migratetype]);
+
+	for (nid = 0; nid < xen_numa_num_nodes; nid++) {
+		pcp = &p->numa_pcp[nid];
+		pcp->count = 0;
+		for (migratetype = 0; migratetype < MIGRATE_PCPTYPES; migratetype++)
+			INIT_LIST_HEAD(&pcp->lists[migratetype]);
+	}
 }

 static void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)
@@ -5497,11 +5590,15 @@ static void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)
 static void pageset_set_high(struct per_cpu_pageset *p,
 				unsigned long high)
 {
+	int nid;
 	unsigned long batch = max(1UL, high / 4);
 	if ((high / 4) > (PAGE_SHIFT * 8))
 		batch = PAGE_SHIFT * 8;

 	pageset_update(&p->pcp, high, batch);
+
+	for (nid = 0; nid < xen_numa_num_nodes; nid++)
+		pageset_update(&p->numa_pcp[nid], high, batch);
 }

 static void pageset_set_high_and_batch(struct zone *zone,
@@ -7755,7 +7852,7 @@ void zone_pcp_reset(struct zone *zone)
 #endif
 		list_del(&page->lru);
 		rmv_page_order(page);
-		zone->free_area[order].nr_free--;
+		zone->numa_free_area[page->machine_nid][order].nr_free--;
 		for (i = 0; i < (1 << order); i++)
 			SetPageReserved((page+i));
 		pfn += (1 << order);
